Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.
#=========================== Test data ==========================#
test_data:  <torch.utils.data.dataloader.DataLoader object at 0x0000017AC6194748>
RegressionModel(
  (embs): ModuleList(
    (0): Embedding(8, 4)
    (1): Embedding(14, 7)
    (2): Embedding(8, 4)
    (3): Embedding(14, 7)
    (4): Embedding(7, 3)
    (5): Embedding(6, 3)
    (6): Embedding(3, 1)
    (7): Embedding(14, 7)
  )
  (lins): ModuleList(
    (0): Linear(in_features=40, out_features=1000, bias=True)
    (1): Linear(in_features=1000, out_features=500, bias=True)
    (2): Linear(in_features=500, out_features=250, bias=True)
  )
  (bns): ModuleList(
    (0): GroupNorm(1, 1000, eps=1e-05, affine=True)
    (1): GroupNorm(1, 500, eps=1e-05, affine=True)
    (2): GroupNorm(1, 250, eps=1e-05, affine=True)
  )
  (outp): Linear(in_features=250, out_features=1, bias=True)
  (emb_drop): Dropout(p=0.04, inplace=False)
  (drops): ModuleList(
    (0): Dropout(p=0.001, inplace=False)
    (1): Dropout(p=0.01, inplace=False)
    (2): Dropout(p=0.01, inplace=False)
  )
  (bn): GroupNorm(1, 4, eps=1e-05, affine=True)
  (activation): Sigmoid()
) 


=== RUN # 0 ====================================

  0%|                                                       | 0/2 [00:00<?, ?it/s]CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\c10\cuda\CUDAFunctions.cpp:100.)
 50%|███████████████████████▌                       | 1/2 [00:02<00:02,  2.84s/it]grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.
grad.sizes() = [3, 1], strides() = [1, 3]
param.sizes() = [3, 1], strides() = [1, 1] (Triggered internally at  ..\torch/csrc/autograd/functions/accumulate_grad.h:170.)
PrivacyEngine expected a batch of size 128 but the last step received a batch of size 98. This means that the privacy analysis will be a bit more pessimistic. You can set `drop_last = True` in your PyTorch dataloader to avoid this problem completely
100%|███████████████████████████████████████████████| 2/2 [00:03<00:00,  2.17s/it]100%|███████████████████████████████████████████████| 2/2 [00:03<00:00,  1.72s/it]
Train Epoch: 1 	Loss: 0.715862 (ε = 7.39, δ = 1e-05) for α = 4.4
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  1.40it/s]100%|███████████████████████████████████████████████| 2/2 [00:01<00:00,  1.44it/s]100%|███████████████████████████████████████████████| 2/2 [00:01<00:00,  1.46it/s]
Train Epoch: 2 	Loss: 0.668139 (ε = 10.79, δ = 1e-05) for α = 3.4
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  1.53it/s]100%|███████████████████████████████████████████████| 2/2 [00:01<00:00,  1.53it/s]100%|███████████████████████████████████████████████| 2/2 [00:01<00:00,  1.52it/s]
Train Epoch: 3 	Loss: 0.657256 (ε = 13.54, δ = 1e-05) for α = 3.0
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  1.89it/s]100%|███████████████████████████████████████████████| 2/2 [00:01<00:00,  1.92it/s]100%|███████████████████████████████████████████████| 2/2 [00:01<00:00,  1.94it/s]
Train Epoch: 4 	Loss: 0.622834 (ε = 15.97, δ = 1e-05) for α = 2.7
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  1.51it/s]100%|███████████████████████████████████████████████| 2/2 [00:01<00:00,  1.68it/s]100%|███████████████████████████████████████████████| 2/2 [00:01<00:00,  1.82it/s]
Train Epoch: 5 	Loss: 0.628179 (ε = 18.19, δ = 1e-05) for α = 2.5
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  1.43it/s]100%|███████████████████████████████████████████████| 2/2 [00:01<00:00,  1.68it/s]100%|███████████████████████████████████████████████| 2/2 [00:01<00:00,  1.91it/s]
Train Epoch: 6 	Loss: 0.642925 (ε = 20.26, δ = 1e-05) for α = 2.4
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.07it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.20it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.29it/s]
Train Epoch: 7 	Loss: 0.671652 (ε = 22.22, δ = 1e-05) for α = 2.3
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.18it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.36it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.46it/s]
Train Epoch: 8 	Loss: 0.698909 (ε = 24.10, δ = 1e-05) for α = 2.2
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.38it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.35it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.33it/s]
Train Epoch: 9 	Loss: 0.728442 (ε = 25.91, δ = 1e-05) for α = 2.2
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.02it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.19it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.30it/s]
Train Epoch: 10 	Loss: 0.745850 (ε = 27.64, δ = 1e-05) for α = 2.1
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.19it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.33it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.43it/s]
Train Epoch: 11 	Loss: 0.765869 (ε = 29.36, δ = 1e-05) for α = 2.1
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.14it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.36it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.52it/s]
Traceback (most recent call last):
  File "main.py", line 314, in <module>
    main()
  File "main.py", line 225, in main
    train(args, model, device, train_data, criterion, optimizer, epoch, s)
  File "C:\Users\rapha\Documents\GitHub\bias-mitigation-sgd\train.py", line 41, in train
    epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(args.delta)
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\privacy_engine.py", line 244, in get_privacy_spent
    rdp = self.get_renyi_divergence() * self.steps
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\privacy_engine.py", line 220, in get_renyi_divergence
    self.sample_rate, self.noise_multiplier, 1, self.alphas
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\privacy_analysis.py", line 259, in compute_rdp
    rdp = np.array([_compute_rdp(q, noise_multiplier, order) for order in orders])
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\privacy_analysis.py", line 259, in <listcomp>
    rdp = np.array([_compute_rdp(q, noise_multiplier, order) for order in orders])
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\privacy_analysis.py", line 234, in _compute_rdp
    return _compute_log_a(q, sigma, alpha) / (alpha - 1)
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\privacy_analysis.py", line 198, in _compute_log_a
    return _compute_log_a_for_frac_alpha(q, sigma, alpha)
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\privacy_analysis.py", line 166, in _compute_log_a_for_frac_alpha
    log_a0 = _log_sub(log_a0, log_s0)
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\privacy_analysis.py", line 81, in _log_sub
    return math.log(math.expm1(logx - logy)) + logy  # expm1(x) = exp(x) - 1
KeyboardInterrupt
