Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.
#=========================== Test data ==========================#
test_data:  <torch.utils.data.dataloader.DataLoader object at 0x000001F8453E82C8>
RegressionModel(
  (embs): ModuleList(
    (0): Embedding(8, 4)
    (1): Embedding(14, 7)
    (2): Embedding(8, 4)
    (3): Embedding(14, 7)
    (4): Embedding(7, 3)
    (5): Embedding(6, 3)
    (6): Embedding(3, 1)
    (7): Embedding(14, 7)
  )
  (lins): ModuleList(
    (0): Linear(in_features=40, out_features=1000, bias=True)
    (1): Linear(in_features=1000, out_features=500, bias=True)
    (2): Linear(in_features=500, out_features=250, bias=True)
  )
  (bns): ModuleList(
    (0): GroupNorm(1, 1000, eps=1e-05, affine=True)
    (1): GroupNorm(1, 500, eps=1e-05, affine=True)
    (2): GroupNorm(1, 250, eps=1e-05, affine=True)
  )
  (outp): Linear(in_features=250, out_features=1, bias=True)
  (emb_drop): Dropout(p=0.04, inplace=False)
  (drops): ModuleList(
    (0): Dropout(p=0.001, inplace=False)
    (1): Dropout(p=0.01, inplace=False)
    (2): Dropout(p=0.01, inplace=False)
  )
  (bn): GroupNorm(1, 4, eps=1e-05, affine=True)
  (activation): Sigmoid()
) 


=== RUN # 0 ====================================

  0%|                                                       | 0/2 [00:00<?, ?it/s]CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\c10\cuda\CUDAFunctions.cpp:100.)
 50%|███████████████████████▌                       | 1/2 [00:02<00:02,  2.88s/it]grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.
grad.sizes() = [3, 1], strides() = [1, 3]
param.sizes() = [3, 1], strides() = [1, 1] (Triggered internally at  ..\torch/csrc/autograd/functions/accumulate_grad.h:170.)
PrivacyEngine expected a batch of size 128 but the last step received a batch of size 98. This means that the privacy analysis will be a bit more pessimistic. You can set `drop_last = True` in your PyTorch dataloader to avoid this problem completely
100%|███████████████████████████████████████████████| 2/2 [00:04<00:00,  2.44s/it]100%|███████████████████████████████████████████████| 2/2 [00:04<00:00,  2.14s/it]
Train Epoch: 1 	Loss: 0.703238 (ε = 7.39, δ = 1e-05) for α = 4.4
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.03it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.20it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.31it/s]
Train Epoch: 2 	Loss: 0.660843 (ε = 10.79, δ = 1e-05) for α = 3.4
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.11it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.20it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.24it/s]
Train Epoch: 3 	Loss: 0.643107 (ε = 13.54, δ = 1e-05) for α = 3.0
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.22it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.32it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.39it/s]
Train Epoch: 4 	Loss: 0.627364 (ε = 15.97, δ = 1e-05) for α = 2.7
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.27it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.38it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.44it/s]
Train Epoch: 5 	Loss: 0.600145 (ε = 18.19, δ = 1e-05) for α = 2.5
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.11it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.21it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.27it/s]
Train Epoch: 6 	Loss: 0.586312 (ε = 20.26, δ = 1e-05) for α = 2.4
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  1.82it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.01it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.15it/s]
Train Epoch: 7 	Loss: 0.565258 (ε = 22.22, δ = 1e-05) for α = 2.3
  0%|                                                       | 0/2 [00:00<?, ?it/s] 50%|███████████████████████▌                       | 1/2 [00:00<00:00,  2.17it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.25it/s]100%|███████████████████████████████████████████████| 2/2 [00:00<00:00,  2.31it/s]
Train Epoch: 8 	Loss: 0.554796 (ε = 24.10, δ = 1e-05) for α = 2.2
  0%|                                                       | 0/2 [00:00<?, ?it/s]  0%|                                                       | 0/2 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 318, in <module>
    if __name__ == "__main__":
  File "main.py", line 229, in main
    for epoch in range(1, args.epochs + 1):
  File "C:\Users\rapha\Documents\GitHub\bias-mitigation-sgd\train.py", line 35, in train
    optimizer.step()
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\privacy_engine.py", line 197, in dp_step
    self.privacy_engine.step()
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\privacy_engine.py", line 280, in step
    self.clipper.clip_and_accumulate()
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\per_sample_gradient_clip.py", line 182, in clip_and_accumulate
    flat=not self.norm_clipper.is_per_layer,
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\utils\tensor_utils.py", line 38, in calc_sample_norms
    norms = [param.view(len(param), -1).norm(2, dim=-1) for name, param in named_params]
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\opacus\utils\tensor_utils.py", line 38, in <listcomp>
    norms = [param.view(len(param), -1).norm(2, dim=-1) for name, param in named_params]
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\tensor.py", line 389, in norm
    return torch.norm(self, p, dim, keepdim, dtype=dtype)
  File "C:\Users\rapha\AppData\Local\Programs\Python\Python37\lib\site-packages\torch\functional.py", line 1337, in norm
    return _VF.norm(input, p, _dim, keepdim=keepdim)  # type: ignore
KeyboardInterrupt
